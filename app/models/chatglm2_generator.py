# app/models/chatglm_generator.py - ä¿®å¤ç‰ˆ
import torch
from transformers import AutoTokenizer, AutoModel
import logging
import re
import os
import sys

logger = logging.getLogger(__name__)

class ChatGLM2Generator:
    def __init__(self, model_path=None):
        """ChatGLM2ç”Ÿæˆå™¨ï¼Œä¸“é—¨é’ˆå¯¹Windows Flaskç¯å¢ƒä¿®å¤"""
        logger.info("åˆå§‹åŒ–ChatGLM2ç”Ÿæˆå™¨ï¼ˆWindows Flaskä¸“ç”¨ç‰ˆï¼‰...")
        
        # ğŸš¨ å…³é”®ä¿®å¤ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¦ç”¨é‡åŒ–å’ŒCUDAæ‰©å±•
        os.environ['USE_CUDA_EXT'] = '0'
        os.environ['DISABLE_QUANTIZATION'] = '1'
        os.environ['LOAD_IN_8BIT'] = '0'
        os.environ['LOAD_IN_4BIT'] = '0'
        
        # è®¾ç½®æ¨¡å‹è·¯å¾„
        if model_path is None:
            # Windowsè·¯å¾„ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
            model_path = r".\model_cache\chatglm2-6b-int4"
            # æˆ–è€…ç”¨ç»å¯¹è·¯å¾„
            # model_path = r"C:\Users\ä½ çš„ç”¨æˆ·å\project\model_cache\chatglm2-6b-int4"
        
        self.model_path = model_path
        self._load_model()
    
    def _load_model(self):
        """ä¸“é—¨ä¸ºWindows Flaskç¯å¢ƒä¼˜åŒ–çš„åŠ è½½æ–¹æ³•"""
        try:
            # ğŸš¨ ä¿®å¤1ï¼šæ·»åŠ å½“å‰ç›®å½•åˆ°sys.path
            current_dir = os.path.dirname(os.path.abspath(__file__))
            sys.path.insert(0, current_dir)
            
            # ğŸš¨ ä¿®å¤2ï¼šåœ¨å¯¼å…¥transformersä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
            import warnings
            warnings.filterwarnings("ignore", message=".*quantization.*")
            warnings.filterwarnings("ignore", message=".*CUDA.*")
            
            logger.info(f"åŠ è½½æ¨¡å‹: {self.model_path}")
            
            # ğŸš¨ ä¿®å¤3ï¼šå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œç¦ç”¨è¿œç¨‹ä»£ç ï¼ˆå¦‚æœå¯èƒ½ï¼‰
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
                raise FileNotFoundError(f"è¯·ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½åˆ°: {self.model_path}")
            
            # ğŸš¨ ä¿®å¤4ï¼šä½¿ç”¨éå¸¸ä¿å®ˆçš„å‚æ•°åŠ è½½
            from transformers import AutoTokenizer, AutoModel
            
            # å…ˆåŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                local_files_only=True,
                revision="main"
            )
            
            # ğŸš¨ ä¿®å¤5ï¼šå¼ºåˆ¶CPUæ¨¡å¼ï¼Œä½¿ç”¨float32ï¼Œé¿å…ä»»ä½•é‡åŒ–
            self.model = AutoModel.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                dtype=torch.float32,  # å¼ºåˆ¶ä½¿ç”¨float32
                local_files_only=True,
                revision="main"
            ).float()  # ç¡®ä¿æ˜¯float32
            
            # ğŸš¨ ä¿®å¤6ï¼šæ˜¾å¼è®¾ç½®ä¸ºCPUæ¨¡å¼
            self.model = self.model.cpu() if torch.cuda.is_available() else self.model
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            logger.info("âœ… ChatGLM2åŠ è½½æˆåŠŸï¼ˆWindows Flaskä¸“ç”¨æ¨¡å¼ï¼‰")
            
            # ç®€å•æµ‹è¯•
            test_response, _ = self.model.chat(
                self.tokenizer,
                "ä½ å¥½",
                history=[],
                max_length=100
            )
            logger.info(f"æ¨¡å‹æµ‹è¯•: {test_response[:20]}...")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            traceback.print_exc()
            raise
    
    def generate_caption(self, image_description, style="é€šç”¨"):
        """ç”Ÿæˆæ–‡æ¡ˆ"""
        try:
            # ç®€å•çš„æç¤ºè¯æ¨¡æ¿
            prompt = f"è¯·å†™ä¸€æ¡{style}é£æ ¼çš„æœ‹å‹åœˆæ–‡æ¡ˆï¼Œå…³äºï¼š{image_description}"
            
            # ç”Ÿæˆ
            with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
                response, _ = self.model.chat(
                    self.tokenizer,
                    prompt,
                    history=[],
                    max_length=80,
                    temperature=0.7
                )
            
            # æ¸…ç†ç»“æœ
            result = response.strip()
            for prefix in ["æ–‡æ¡ˆï¼š", "æ–‡æ¡ˆ:", "å¥½çš„ï¼Œ", "ä»¥ä¸‹æ˜¯"]:
                if result.startswith(prefix):
                    result = result[len(prefix):].strip()
            
            return result
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥: {e}")
            # ç®€å•çš„åå¤‡
            return f"è®°å½•ï¼š{image_description}"